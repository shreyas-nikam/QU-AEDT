Hello! Welcome to module 4 of the NIST's Adversarial Machine Learning course. In this module, we will be talking about the discussion and the remaining challenges of Adversarial Machine Learning.

As we move forward in our discussion, let's focus on the Scale Challenge. One of the main issues is that as our models grow, so too does the amount of training data. This is a significant issue given that many of the leading Large Language Models or LLMs utilize a gigantic amount of data. It's been observed that the latest GenAI systems are requiring even larger amounts of data which are distributed across various modalities. Now, it's imperative to understand that no single organization or even a nation owns the complete data that is used for training an LLM. Modern data repositories are not simply single units of data but rather they consist of labels and links connected to servers holding the actual data. This creates a new set of cybersecurity risks that we must address. We must also be aware of other scale-related problems such as the capacity to produce synthetic content at scale. This has possible negative impacts on LLMs and their capabilities. Therefore, tackling these scale challenges is a necessity for the future development of foundation models and their alignment with our values.

Let's dive into our discussion and remaining challenges, focusing on issues of data poisoning and synthetic content. Open-source data poisoning tools are on the rise and these tools carry a significant risk of large-scale attacks on image training data. While the intention of these tools is to protect copyright, particularly for artists, misuse can lead to detrimental consequences. We also need to acknowledge the challenges posed by the availability of powerful models which can generate massive amounts of unmarked synthetic content. While watermarking might be a potential solution, unmarked synthetic content can severely impact subsequently trained Large Language Models, or LLMs, potentially leading to model collapse. Both these issues require our attention and pose significant challenges moving forward.

First of all, designing mitigations for attacks on AI systems proves challenging due to the current lack of information-theoretically secure machine learning algorithms. This is a major obstacle we need to overcome in order to ensure the safety and reliability of AI systems in crucial domains. 

Secondly, another key challenge facing the AML field is the ability to detect when the model is under attack. This is critical, as identifying an attack in its early stages gives us the opportunity to counteract it, preventing potential information loss or triggering of adverse behaviour in the model. Techniques to detect adversarial examples are equivalent to robust classification, yet this is difficult to construct and represents an ongoing challenge in the field.

Finally, the detection of out-of-distribution inputs, or OOD inputs, is another significant challenge we face in AML. Adversarial examples may originate from the expected data distribution, or they might represent OOD ones, both of which our models need to be prepared for. Focusing on these major challenges will ensure that we make progress and pave the way for safer AI deployments.

Next up, we're going to discuss Sanitization and Security Measures. We'll start by talking about data and model sanitization techniques. These techniques have the potential to reduce the impact of poisoning attacks. Important to note, however, is that cryptographic techniques should be used in tandem with these sanitization methods for origin and integrity attestation, according to the National Security Commission on AI. 

Next, let's delve into the subject of prompt injections. These are specific attacks targeted at chatbots, necessitating stringent regulation to prevent detrimental behavior. It's essential to realize that there are limitations to this approach, indicating the need for implementing additional cybersecurity mechanisms.

Lastly, I'd like to highlight the growing development of AI-enabled chatbots. As these become increasingly prevalent, it is critically important to incorporate risk management throughout the technology life cycle. In tandem with this, comprehensive pre-deployment testing is needed before these chatbots are put into operation, given the potential risks associated. We'll move on with a closer look at these points later in the presentation.

Turning our focus to the next topic, we are discussing the topic of Chatbots and their associated Perceived Risks. We know that technology, such as chatbots, is still emerging. It has significant potential but should be used carefully, especially in applications that rely on high trust. There must be continuous monitoring in place. 

As the use of chatbots becomes more prevalent online, adversaries are constantly seeking new ways to discover and exploit vulnerabilities. It's a continuous race, tech companies on one hand are working tirelessly to improve designs and safeguard against these attacks. 

Major risks that need to be identified and mitigated include bias and discrimination. This technology is capable of generating harmful content and it can also violate privacy. These risks are significant and require careful attention and mitigation strategies. Remember, the use of this technology is a trade-off, we must evaluate the risks versus the benefits in order to make informed decisions.

Moving on, we delve into 'Robust Training and Standardized Benchmarks'. We acknowledge that robust training techniques offer varying approaches to theoretically certified defenses against data poisoning attacks. However, this field still requires more research particularly in making these techniques handle out-of-distribution inputs and large-scale models. In the area of adversarial machine learning mitigation testing, we've observed a lack of reliable benchmarks. This situation makes proposed mitigations incomparable, therefore, the development of standardized benchmarks is absolutely vital for gaining reliable insights. Despite the notable cost associated with formal methods verification, such techniques can provide security and safety assurances, these are particularly essential in high-risk applications. Therefore, as we continue to rapidly advance in the space of AI and machine learning, the investment in improved training techniques, developing standardized benchmarks and the inclusion of formal verification methods, will be fundamental in addressing the diverse set of challenges we face.

Let's delve into the main points of Imbalance Between Privacy Attacks and Mitigation Techniques. The rapid advancement of AI technology is currently outpacing the development of mitigation techniques. While AI progresses at lightning speed, it also leaves behind trails of weaknesses that can potentially expose privacy vulnerabilities and attract more adversaries. Our main challenges lie in finding efficient ways to mitigate potential exploits of memorized data and preventing inference of training data membership or other properties. There is also the pressing need to protect machine learning models from intellectual property theft. As we find answers to these challenges, it's crucial that tailored guidelines be created. These guidelines should be fully encapsulating and effective, especially in scenarios where privacy is of utmost importance.

We'll be diving into the discussion and the remaining challenges surrounding the Open vs. Closed Model Dilemma. Open source software development has proven itself to be an indispensable methodology in modern times. It provides numerous benefits like democratizing access, leveling the playing field, and most importantly, enhancing scientific reproducibility. These advantages make open source a robust tool that closes performance gaps seen in closed models.

However, every advantage comes with its own set of concerns. There are worries over the potential misuse of the open AI technology by individuals or groups with malicious intent. This raises a critical question: Should we allow the unrestricted use of open models? The answer to this question is complex and it's a topic that the community of stakeholders continues to actively debate.

Let's also look at how similar questions have been explored in other fields. For instance, in cryptography, the associated risks have been accepted by society, leading to the creation of strong, publicly available cryptographic algorithms. On the other hand, the risks involved in bioengineering are seen as too severe and therefore, open accessibility to the technology is not allowed. Now let's consider artificial intelligence. The open vs. closed model dilemma in AI is a hot topic of debate among stakeholders. It is crucial that we find a solution before these models become too powerful and uncontrollable. The question of whether to allow open models is a complex one. The cases of cryptography and bioengineering illustrate how differently these situations can play out. As we move forward, we must carefully consider both the potential benefits and risks of AI models, to ensure the best outcome for all concerned.

It’s also important that we focus on the challenges within Supply Chain Challenges. By observing trends in AML literature, we're finding that new attacks are being designed with increasingly high power and stealthier behavior. This is bringing about significant challenges to applications utilizing open models throughout the supply chain. It's crucial to note here that backdoor attacks on models are a reality, and they're not limited to open models. 

Moving on, DARPA, in collaboration with NIST, launched a program named TrojAI. This program concentrates on the defense of AI systems from intentional, malicious Trojan attacks by developing effective technology to detect these attacks. It also delves into understanding what makes Trojan detection a challenging task in its essence.

Finally, let's discuss a new class of attacks - information-theoretically undetectable Trojans. These can be planted in ML models and, if proven practical, could only be preventable or detected and mitigated by procedures to thoroughly vet third-party components. Additionally, strict control over access to the model throughout the life cycle may be necessary. The NIST AI Risk Management Framework provides further comprehensive details.

We're going to explore the concept of understanding the tradeoffs between the attributes of trustworthy AI. The trustworthiness of an AI system isn't determined solely by accuracy but by all its characteristics. We have to take into account potential vulnerabilities or biases that an AI system may have, regardless of their accuracy. These could detrimentally affect how much trust we put into the system. 

Moreover, there's a delicate balance that exists between various AI attributes. For example, when we look at explainability and adversarial robustness, or privacy and fairness, there could be trade-offs. Focusing on optimizing AI for one attribute could result in underperformance in others. 

It's crucial to be aware that the exact portrayal of trade-offs between different attributes of trustworthy AI isn't fully understood yet. This is an open research problem that's growing increasingly significant due to the surging use of AI technology. 

So, as we further adopt AI into multiple aspects of our lives and various industries, gaining a deep understanding of these tradeoffs and how we can strike an optimal balance between them is of paramount importance.

Now, we are shedding light on the crucial role organizations play in managing the trade-offs that come with implementing trustworthy AI. Navigating these trade-offs is a complex task that organizations need to tackle decisively, as they differ based on the AI system, the use case, and an array of considerations. The considerations take into account the economic, environmental, social, cultural, political, and global implications of the AI technology. Taking into consideration, for instance, an AI system that is highly accurate, but easily susceptible to adversarial exploits would not be likely to gain trust. Moreover, an equally crucial trade-off to be managed is between AI's explainability and its susceptibility to adversarial actions. We also cannot ignore the trade-offs between privacy and fairness, especially in situations where both these factors are paramount. Importantly, these trade-offs are a reality because it is currently impossible to maximize an AI system's performance across these distinctive aspects simultaneously. Understanding the various trade-offs inherent in AI decision-making helps us understand why an organization's role in managing these trade-offs becomes so indispensable.

Let's delve into the discussion and the remaining challenges we face in the field of multimodal models. Despite their high potential for outstanding performance across various machine learning tasks, these models present a crucial issue: their robustness against adversarial perturbations within a single modality. This means that even with the availability of redundant information across different modalities, these models aren't necessarily robust against these perturbations. 

We have witnessed how effective defenses against evasion attacks, which are based on adversarial training, are not economically feasible for multimodal learning applications. Hence, it's crucial to find a way to utilize the redundant information to improve the robustness against these single modality attacks.

We must also consider that researchers have developed efficient mechanisms to construct simultaneous attacks on various modalities. This suggests that multimodal models might not be more robust against adversarial attacks, even with improved performance. It becomes apparent then that mitigation techniques dependent solely on single modality perturbations aren't likely to be effective or robust. 

This presents major challenges that our field must address to ensure the robustness and efficiency of multimodal models.

Let's discuss some additional considerations on multimodal models. It has been observed that simultaneous attacks on these models exist. This suggests that mitigation techniques, if they only concentrate on single modality perturbations, may not be robust or adequate in handling the attacks. Reflecting on real-life scenarios, attackers do not limit themselves to attacks within a prescribed security model; instead, they take advantage of any available attack method. This underlines a crucial point: multimodal models may not provide enhanced performance against these adversarial attacks despite their perceived advantages. So, while multimodal models showcase great potential, it's essential to recognize their limitations and vulnerabilities in an environment that's rife with unpredictable and varied attack methods.

Let's move on to the discussion and remaining challenges related to quantized models. Now, we've understood that quantization is a method deployed extensively for implementing models to edge platforms. This could vary from smartphones to IoT devices. The primary advantage of implementing quantization is that it helps reduce computational and memory costs by utilizing low-precision data types. However, there's a flipside to it. It is important to be aware that quantized models inherit the vulnerabilities of the original models and can potentially bring in additional weaknesses. This additional weakness makes the models susceptible to adversarial attacks. These adversarial attacks could be due to error amplification, which is a result of reduced computational precision. Mitigation techniques do exist for PredAI models, but the effects of quantization on GenAI models are somewhat unknown territory and have been less explored. Hence, I would recommend companies who are deploying such models to be mindful of continuously monitoring their behavior.

Congratulations! You have completed the module 4 on Discussion and Remaining Challenges. With the completion of this module, you have completed the course on NIST’s Adversarial Machine Learning offered by QuantUniversity.  For any questions you have, check out the chatbot, the attached slides. Once complet, take the competency quiz and get a certificate of completion from QuantUniversity! Check out our other courses on related topics at www.quantuniversity.com