Hello! Welcome to module 2 of the NIST's Adversarial Machine Learning course. In this module, we will be introducing you to the Predictive AI Taxonomy in Adversarial Machine Learning and we will be discussing Attack Classification, Evasion attacks, their mitigation, poisoning attacks and their mitigation and the privacy attacks.

Today we deep dive into the mechanics of Predictive AI Taxonomy and Attack Classification. The taxonomy of attacks in adversarial machine learning for PredAI systems directly focuses on three key objectives - breakdown of Availability, violations of Integrity, and compromise of Privacy. It's fascinating to see how attackers employ diverse capabilities to strike their objectives successfully. Notably, it is vitally crucial to distinguish between individual attack classes and their required capabilities to achieve a specific objective.

Moving forward, our discussion would be incomplete without delineating the attack classifications which are highly dependent on varying dimensions. A few noteworthy examples include the learning method and stage when the attack is administered, what goals and objectives the attacker has in mind, what capabilities the adversary possesses and lastly, the attacker's knowledge about the learning process itself. 

The ultimate goal of this courseis not simply to classify and categorize - it goes beyond that. It provides a framework aiming to establish a standard terminology for adversarial attacks on machine learning, which essentially unifies all the existing work related to this domain.


The above figure introduces a taxonomy of attacks in adversarial machine learning. The attacker’s objectives are shown as disjointed circles with the attacker’s goal at the center of each circle: Availability breakdown, Integrity violations, and Privacy compromise. The capabilities that an adversary must leverage to achieve their objectives are shown in the outer layer of the objective circles. Attack classes are shown as callouts connected to the capabilities required to mount each attack. Multiple attack classes that requiring same capabilities for reaching the same objective are shown in a single callout. Related attack classes that require different capabilities for reaching the same objective are connected with dotted lines.

We begin our exploration by understanding the two primary stages in machine learning. The first one is the training stage where a model learns from the data fed into it. Following this is the deployment stage where this trained model generates predictions. Now, there are several types of learning methodologies that come into play. Supervised learning utilizes labeled training data whereas unlabeled data is used in unsupervised learning. Further, we come across other learning types such as semi-supervised, reinforcement, federated and ensemble learning. 

As we move forward, let's talk about the ML models which can be generative or discriminative. Generative models learn the distribution of the training data, whereas, on the contrary, discriminative models learn a decision boundary. It's interesting to note that most predictive AI models are discriminative in nature.

Now, let's look at the adversarial aspect of machine learning. This is where potential attacks against AI systems are considered, which could occur at either the training or deployment stage of the model. The attackers might manipulate parts of the training data, their labels, the model parameters or even the machine learning algorithms code. So, as you can see, understanding the taxonomy and classification of potential vulnerabilities is crucial for designing secure AI systems.

Let's take a look at the different types of attacks associated with machine learning. During the training stage, we face what are called poisoning attacks, which can come in two forms: data poisoning and model poisoning. If an adversary gets control over a subset of the training data, it leads to a data poisoning attack. On the other hand, if the adversary gets control over the model and its parameters, it's a model poisoning attack. It's crucial to understand that data poisoning applies to all learning paradigms, while model poisoning is more prevalent in federated learning and supply-chain attacks.

Then we have attacks that happen during the deployment stage. These primarily include evasion and privacy attacks. Evasion attacks seek to create adversarial examples, affecting the ML model's predictions. Privacy attacks, on the other hand, aim to infer sensitive information. They can further be divided into data privacy attacks that target the training data, and model privacy attacks that target the information about the ML model. Given these threats, understanding and mitigating these attacks is crucial for maintaining the security and reliability of our machine learning models.

Our discussion next focuses on attack classification. Specifically, we'll be classifying attacker objectives into three main dimensions that represent the different types of security violations in a system - availability, integrity, and confidentiality. When an attacker achieves one or more of these objectives, it means they have been successful in their adversarial attempt. Let's take a closer look at each of these dimensions. 

Firstly, availability pertains to attempts to hamper or entirely break down the performance of a Machine Learning model during deployment. These types of attacks can be executed via data poisoning, model poisoning, or even through energy-latency attacks.

The second category, integrity, focuses on the targeting of an ML model's output, leading to the model making incorrect predictions. An attacker can achieve this by creating adversarial examples that are misclassified by the model while remaining imperceptible to humans. 

Lastly, confidentiality or privacy attacks are geared towards gaining unauthorized information about the training data or the ML model. These pernicious attacks aim to compromise the privacy of the training data or extract information about the model itself. 

Understanding these different dimensions of attacks helps us to identify the potential breach points in a system and devise ways to countermeasure such attacks.

Let's dive into the types of attacks an adversary might perform when breaching the security of machine learning systems. The first type is an 'Availability Breakdown'. This occurs when an attacker targets a machine learning model and attempts to reduce its performance during deployment, marking a significant threat to availability. The tactics they might employ to perform this attack could be through data poisoning, where a portion of the training data set is under the attacker's control, or model poisoning, which involves controlling the parameters of the model itself. There is also the possibility of conducting energy-latency attacks. 

The second type is 'Integrity Violations'. As the name suggests, these types of attacks aim to affect the integrity of the outputs produced by an ML model, which results in the model generating incorrect predictions. The attacker could orchestrate an evasion attack at deployment time or a poisoning attack during when the model is training. Evasion attacks involve the modification of test samples to create adversarial examples that the model misclassified, whilst poisoning attacks involve manipulating the integrity of the training data. Understanding these risks helps us to better develop strategies to prevent such occurrences and to protect the integrity of our machine learning systems.

The third type is Privacy Compromise. This is a situation where attackers might show interest in learning things about the training data or about the machine learning model. Now, how might they jeopardize the privacy of training data? They could do this through a range of methods, such as data reconstruction, where they attempt to infer content or features of training data. They might also launch membership-inference attacks, which involve inferring the presence of data in the training set. Another possibility is data extraction, where the attackers try to extract training data from generative models. They might also carry out property inference, where they strive to infer properties about the training data distribution. We should also be aware of a particular model privacy attack called model extraction. In this scenario, the aim of the attackers is to extract information about the machine learning model. So, it's crucial to be aware of these various types of privacy compromise attacks and endeavor to protect against them.

We're now going to understand the taxonomy of predictive AI specifically relating to attacker capabilities. Basically, there are six types of capabilities that an adversary, or attacker, might employ to achieve their objectives. 

First, let's talk about training data control. Here, an attacker has the ability to control a subset of the training data. They can do this by inserting or modifying training samples and this technique is often used for what's known as poisoning attacks.

The second capability is model control. An attacker might control model parameters in a couple of ways - they could generate a Trojan trigger and insert it in the model or by sending malicious updates to the local model in an approach known as federated learning. 

Lastly, for now, we have testing data control. In this situation, an attacker is able to introduce perturbations to testing samples when the model is being deployed. These can be utilized for evasion attacks to create adversarial examples or for backdoor poisoning attacks. Next, we will look into other types of attacker capabilities. Remember, these are just some ways in which an adversary might try to exploit predictive AI systems.

One way attackers can compromise the model is through source code control. It means they can alter the source code of the machine learning algorithm. They can target components like the random number generator or pull in third-party libraries which are frequently open source, to manipulate the results.

Another strategy used by attackers is label limit. This capability is relevant to restrict the adversarial control over the labels of training samples in supervised learning. Clean-label poisoning attacks assume that the attacker does not control the label of the poisoned samples – a realistic poisoning scenario, while regular poisoning attacks assume label control over the poisoned samples.

The last strategy is Query Access. When the ML model is managed by a cloud provider (using Machine Learning as a Service – MLaaS), the attacker might submit queries to the model and receive predictions (either labels or model confidences). This capability is used by black-box evasion attacks, ENERGY-LATENCY ATTACKS, and all privacy attacks.


It's important to note that even without the capability to modify training or testing data, source code or model parameters, just having access is crucial. This access alone can pave the way for white-box attacks. So creating defensive measures is crucial even at the access level.

We now delve into the classification of attacks in the predictive AI taxonomy. An important fact we have discovered is that various attack classes are strongly connected with the capabilities required to initiate the specific attack. As an illustration, let's consider backdoor attacks that cause integrity violations. The attacker needs to gain control of both training and testing data to be able to successfully insert a backdoor pattern. There is another way that backdoor attacks can be orchestrated. This can be achieved via controlling the source code and this is particularly likely when the training process is outsourced. Turning to another variant of these attacks, Clean-label backdoor attacks operate a little differently. In this case, the attacker does not have label control over the poisoned samples. However, they still require the capabilities essential for backdoor attacks. The purpose of understanding these classifications and the capabilities required to initiate these attacks is help us better guard our AI models against potential threats.

Let's delve into how we classify attacks in Predictive AI Taxonomy based on how well an attacker might know the ML system. Firstly, we have White-box attacks. Here, the attacker is entirely knowledgeable about the ML system. They know all about the data, architecture, and hyper-parameters. While this provides the attacker with much power, analyzing such attacks gives us insights into potential vulnerabilities in the system and helps us plan for mitigative tactics. 
Secondly, we discuss Black-box attacks. Contrastingly, these attacks assume the attacker only knows the bare minimum about the ML system. They might have query access to the model, but no further information about the model's training. Essentially, these attacks leverage system interfaces meant for regular usage. Predominantly, this is the most practical type of attack, given the minimal knowledge assumption about the ML system.
Lastly, we have Gray-box attacks, which are a bit of both worlds. Such an attack would mean that the attacker might know the model's architecture but lack knowledge about its parameters or otherwise. A gray-box attack might also take place under the pretense that the attacker has access to data identically distributed to the training data and knows about feature extraction. Understanding feature extraction is crucial in industries such as cybersecurity, finance, and healthcare. In all, the type of attack highly depends on how much knowledge the attacker has about the ML system.

It is important that we examine the classification of adversarial attacks on ML with regard to the data modalities used in different application domains. Such attacks can occur across diverse modalities, including image, text, audio, video, cybersecurity, and tabular data.

Focusing on image data, typical attacks with continuous domain advantage exploit direct optimization strategies such as backdoor poisoning attacks. Additionally, privacy attacks are quite common in image datasets and these attacks encompass various types of imaging techniques. 

When we look at the text modality, attacks can manifest in different forms. In the case of Natural Language Processing or NLP, attacks range from evasion, poisoning to even privacy breaches. 

Moreover, audio systems and even text generated from audio signals have not been immune to such adversarial attacks. Overall, understanding the nature of these varied attack classifications can enhance our perspective on how to guard against them in the respective application domains.

As we continue the discussion from our previous section on Predictive AI Taxonomy - Attack Classification, let's first revisit the vulnerabilities in different modalities. We have observed that video comprehension models, despite their impressive performance on vision-and-language tasks, are still susceptible to attacks. Moreover, in the cybersecurity realm, poisoning attacks were initially identified, but there have been numerous instances involving malware classification, PDF malware classification, and Android malicious app detection since then. In the context of tabular data, we have seen a variety of attacks against machine learning models used in finance, business, and healthcare applications. 

As we delve deeper into multimodal data applications in machine learning models, it's worth highlighting that while they do provide some resilience against attacks, they can also be vulnerable, especially when multiple modalities are attacked simultaneously. This brings us to one of the pressing challenges we face - testing and measuring the resilience of these multimodal machine learning models against evasion, poisoning, and privacy attacks. Therefore, an objective moving forward is to better understand and bolster our defenses against this wide spectrum of threats.

We're going to examine evasion attacks and mitigations in predictive AI taxonomy. Evasion attacks are particularly interesting as they involve the creation of adversarial examples, which are essentially minimal perturbations of testing samples. The aim is to misclassify these adversarial examples. Intriguingly, these examples were initially demonstrated within linear classifiers for spam filters, and later, their relevancy was explored in the field of image classification. 

One of the key challenges in image classification is to ensure that the perturbation is so minor that humans do not notice the change, tricking machine learning algorithms while remaining seemingly unchanged to the human eye.

In 2013, effective methods for producing such adversarial examples against linear models and neural networks were discovered. These methods utilized gradient optimization on an adversarial objective function. Some of these methods required white-box access to the model, but they have been improved by methods that use smaller perturbations.

Interestingly, adversarial examples aren't only relevant in white-box scenarios, they are also considerably applicable in black-box settings where attackers only have query access to the model. In such settings, different tactics like zeroth-order optimization, discrete optimization, Bayesian optimization, and transferability are employed.

Mitigating these adversarial examples presents a well-known and substantial challenge. Defenses have been published before, but more powerful attacks have managed to break them. Some of the most promising mitigation methods include adversarial training, randomized smoothing, and formal verification techniques, however, these methods do have their own limitations. There is a trade-off to consider between robustness and accuracy, and between robustness and fairness guarantees.

Next, let's explore optimization-based techniques for white-box evasion attacks. Adversarial examples are generated using optimization methods, allowing the attacker to determine the target class in targeted attacks or any incorrect class in untargeted attacks. For example, the L-BFGS method was employed by one group, and another group, in dealing with binary classifiers featuring malicious and benign classes, utilized a differentiable discriminant function. An alternative approach is the Fast Gradient Sign Method, particularly applied in the context of deep learning models. You can learn more about these in the original paper.

Various methods and objectives have been employed to minimize perturbations and assess these alterations using distance metrics. Noteworthy attacks in this realm include DeepFool, the Carlini-Wagner attack, and the Projected Gradient Descent attack. The choice of method is contingent on the specific situation, but the overarching aim is to generate adversarial examples capable of misleading the ML algorithm into making incorrect classifications or predictions.

We will now discuss the different types of Evasion Attacks. The first type we are discussing is Universal Evasion Attacks, which entail the creation of small, universal perturbations that can be added to most images, leading to misclassification. This was elegantly demonstrated in a study by Moosavi-Dezfooli et al. which you can refer to in the original paper.

The next type is what we call physically realizable attacks. These are attack variants possible to realize in the physical world. A class example of this category of attacks is the eyeglass frame printing technique which is capable of evading or impersonating facial recognition systems. Other examples of physically realizable attacks include ShapeShifter attack and an attack employing sticker application to evade road sign detection. 

It's worth mentioning that evasion attacks extend beyond computer vision applications. Various domains such as audio, video, natural language processing, and cybersecurity have seen significant development of adversarial examples built with the sole purpose of deceiving machine learning classifiers.

Further, we delve into the discussion on evasion attacks across different data modalities. In the realm of audio, a targeted attack on models that transcribe text from speech, generates an audio waveform quite identical to an existing one but could be transcribed into any text of the attacker's choosing. Moving on to video, adversarial evasion attacks can either affect a small number of video frames, also known as sparse attacks, or each frame in a video, referred to as dense attacks. 

Turning our attention to natural language processing or NLP, adversarial examples have to comply with text semantics. There was pioneering work that paved the way for methods that formalize perturbation definitions with the introduction of the concept of semantic robustness. This is beyond the fact that being the domain discrete, adversarial examples should be meaningful in terms of text.

Lastly, we look at evasion attacks in the cybersecurity domain. Due to constraints imposed by application semantics and feature representations, adversarial examples need to respect these boundaries. This has led to the development of innovative methods such as FENCE and others that leverage formal logic to understand feature space constraints. 

These evasion attacks across different data modalities are optimized-based methods, universal evasion attacks, and physically realizable attacks. What they all share in common is their potential to produce adversarial examples at small distances from the original testing samples but yet capable of faking the AI models. This alarming reality is what makes understanding these attacks crucial for our ongoing and future work in AI security.

Predictive AI Taxonomy delves deep into Black-Box Evasion Attacks, a type of adversarial model. Here, we understand that the adversary doesn't have any prior knowledge about the model architecture or its training data. However, they can interact with the system by querying the trained ML model on different data samples. Black-box evasion attacks are broadly classified into two categories: Score-based attacks and Decision-based attacks. With score-based attacks, the model's confidence scores are utilized by the attackers who create adversarial examples through optimization techniques like zeroth-order optimization, natural evolution strategies, and random walks. Whereas with Decision-based attacks, attackers get only the predicted labels of the model. For manipulation, various techniques such as Boundary Attack, OPT attack, Sign-OPT attack and Bayesian optimization come into play. The biggest challenge faced in creating adversarial examples in black-box settings is the reduction in the number of queries to the ML models. However, current techniques have managed to evade ML classifiers within fewer than 1000 queries.

In the realm of Predictive AI Taxonomy,it is important to discuss the analysis of evasion attacks and mitigations via transferability of attacks. This interesting concept involves generating adversarial attacks under restrictive threat models. One important technique used is training a substitute ML model, generating white-box adversarial attacks on this model, and then transferring the attacks to the target model. You'll find that methods differ significantly in training the substitute models, with some processes using score-based queries, while others train an ensemble of models without querying the target model. 

Now, attack transferability itself is quite an intriguing phenomenon and has been extensively studied. Researchers are interested in understanding why adversarial examples transfer across models. Observations have shown that intersecting decision boundaries in both benign and adversarial dimensions in different models can lead to improved transferability. The two key factors that have been identified to contribute to this transferability are the intrinsic adversarial vulnerability of the target model and the complexity of the surrogate model that is used to optimize the attack. 

Lastly, we will introduce the concept of Expectation Over Transformation. This concept aims to create adversarial examples that can withstand real-world image transformations, such as changes in angle and viewpoint. As we will see, these aspects play a critical role in shaping the robustness and adaptability of adversarial examples.

Here, we are taking a closer look at evasion attacks in the context of predictive AI taxonomy, and the potential mitigations against such attacks. Evasion attacks pose a significant challenge due to adversarial examples pervasively appearing across various ML models and domains. We will focus on the defenses against adversarial evasion attacks, categorized under three robust classifications. The first one is Adversarial Training, an interesting approach that broadens the training data by incorporating adversarial examples. This inclusion leads to a more resilient model capable of withstanding adversarial evasion attacks. However, note that this could also hamper the model's accuracy. 

The second method, known as Randomized Smoothing, converts any classifier into a smooth, robust classifier. It offers proven robustness for evasion attacks under the L2 norm. 

Finally, we have the method of Formal Verification, using formal methods to certify a neural network's robustness against adversarial attacks. 

While these methods offer significant defenses against evasion attacks, it's crucial to note that they also present some trade-offs. The robustness they provide may come with higher computational costs and can potentially hinder model accuracy. It is important to consider these aspects when choosing an approach to mitigate evasion attacks.

We further delve into a subsection of predictive AI taxonomy focused on poisoning attacks and mitigations. Poisoning attacks are a significant threat during the training stage of machine learning systems. They have a widespread application history, with prominent roots in the field of cybersecurity. Interestingly, the first known instance of a poisoning attack was traced back to a situation involving worm signature generation in 2006. In recent times, close attention has been paid to instances of these attacks occurring in industrial applications. A notable report from Microsoft placed these attacks at the top of the list of critical vulnerabilities for deployed machine learning systems.



It's important to understand that there are various types of poisoning attacks that can lead to violations of system availability or integrity. Each differing attack involves a wide array of adversarial abilities. Let's look at availability poisoning attacks, which have a widespread effect causing model degradation on all samples. On the other hand, there are the more elusive targeted and backdoor poisoning attacks, which cause integrity violations on specified sample targets.

It’s also important to look into the concept of Predictive AI Taxonomy, particularly focusing on Poisoning Attacks and Mitigations. We will take an in-depth look at how poisoning attacks utilize several adversarial capabilities which include data poisoning, model poisoning, label control, source code control, and test data control. These capabilities generate different subcategories of these types of attacks. 

We'll also discuss the contexts in which these attacks are found. They often occur in multi-faceted adversarial scenarios such as white-box, gray-box, and black-box models. The discussion will also explore various threat types that are classified according to the targeted adversarial objective. The availability of poisoning, targeted poisoning, backdoor poisoning, and model poisoning attacks are a few of the threats that we need to be aware of. 

Moreover, we'll discuss the methods for mounting such attacks. It's important to know these methods so as to better understand how they function, but also to identify ways to mitigate these threats. The limitations of existing mitigations will also be extensively discussed. 

Unfolding poisoning attacks and corresponding mitigations for Predictive AI Taxonomy is the next part of our discussion. We look first at Availability Poisoning, a type of attack keenly seen in cybersecurity applications that are known to orchestrate denial of service attacks. These are particularly noted in worm signature generation and spam classifiers. These cleverly crafted attacks mislead the worm signature generation algorithm, resulting in the misclassification of spam emails. Despite ML-based methods innovatively employed to detect such attacks, there's still a risk that the training data could be mimicked, leading to the poisoning of the learning process.

Next, we pivot to Label flipping, a simple yet crucial black-box poisoning attack. The interesting fact about this attack is that it requires a considerable number of poisoning samples to launch an attack, an aspect improved by optimization-based poisonings. These were initially introduced against Support Vector Machines. However, it's worth mentioning that these poisoning attacks require white-box access to both the model and training data for generating those poisoning samples.

Lastly, we touch upon clean-label poisoning attacks, widely recognized in scenarios where varying files can be submitted to threat intelligence platforms. The labeling is carried out using anti-virus signatures. The challenge with these attacks lies in their detection, which can be solved by monitoring the standard performance metrics of ML models such as precision, recall, accuracy, F1 scores, and the area under the curve.

"Our current topic revolves around strategies to mitigate the impact of poisoning attacks. The first strategy involves training data sanitization. These methods focus on purifying the training data by dismantling poisoned samples before starting the machine learning training process. This measure also places importance on shielding our datasets employing cybersecurity mechanisms. One of the techniques used in this process is the Region of Non-Interest method, which works by neglecting samples that could potentially reduce the model's accuracy when incorporated. In addition, these methods use label cleaning to fix label flipping attacks. Some strategies also utilize outlier detection and clustering methods to help pinpoint poisoned samples. 



The second strategy involves robust training, which essentially modifies the ML training algorithm. Rather than using regular training, we employ robust training that provides a safeguard against label flipping attacks. Techniques borrowed from robust optimization such as trimmed loss function and applying randomized smoothing to add noise during training, are commonly used. The process of generating predictions is strategic and involves model voting, error calculation and the use of lift measurement.


Now, we delve into the realm of Predictive AI Taxonomy, focusing on Poisoning Attacks and their Mitigations, specifically on Targeted Poisoning. Let's discuss how targeted poisoning attacks work. These attacks induce a change in a Machine Learning model's prediction on a specific set of samples. This change is usually brought about by manipulating the labeling function of the training data. Now, there are several techniques to execute such attacks. For instance, Influence Functions, and optimization based on feature collision leverage clean-label settings for the attack. Another example is the subpopulation poisoning attacks, which targets samples from an entire subpopulation. You can learn more about the working of these attacks in the original paper.

Navigating to how we can mitigate these attacks. Mitigations for targeted poisoning attacks, I must say, are challenging. One recommended approach is the use of cybersecurity mechanisms for dataset provenance and integrity attestation. Further, Differential Privacy or DP is proposed as a defense, but it's critical to remember that there's a trade-off between robustness and accuracy with this method. As we move forward in our broader discussion on Predictive AI Taxonomy, we will continue to explore more about these attacks and mitigation methods.

Backdoor poisoning attacks were first created with the conception of BadNets. You might find it interesting that these backdoor poisoning attacks have gradually become sophisticated, making them quite challenging to identify. Their usage is not confined to a specific area, they've been effectively used across different contexts, such as Audio, NLP, and cybersecurity. For instance, backdoors catering to sentiment analysis and neural machine translation applications were created and are called as semantic-preserving backdoors.

Let's move on to discuss some of the most recent attacks. Technology has advanced to the point that it now allows for dynamic localization of the trigger or even altering the functional triggers based on the input. An innovative approach involves using physical objects like sunglasses and earrings as the poison for facial recognition systems. Another noteworthy instance is introducing a clean-label attack that uses natural reflections as a trigger on images. 

Now, although we have talked about these attacks, it's equally important to have methods in place to mitigate or defend against them. A few defense strategies used involve training data sanitization, trigger reconstruction, and model inspection and sanitization. However, it's easier said than done given that the recent incorporation of semantic and functional backdoor triggers poses significant challenges to these approaches.

Let's discuss advanced mitigations and future research in the context of Backdoor Poisoning. Advanced mitigations include techniques such as outlier detection in the latent feature space and activation clustering. These work by isolating backdoored samples in a separate cluster. There are also methods that involve reconstructing the backdoor trigger or analysing the trained model to determine if it is poisoned. System sanitization can then be performed through various means such as pruning, retraining or fine-tuning the model.

However, there are limitations to current defense strategies. They often fail against clean-label backdoor poisoning instances on malware classifiers. The training stage of meta classifiers is also computationally intense. These limitations point to the need for further research to devise strong backdoor attack mitigations.

In addition to these mitigations, poison forensics can provide an added layer of defense in an ML system. This technique helps in identifying the malicious training examples and tracing back the source of attack in the training set. This way, we can not only mitigate a backdoor attack but also learn about its origin, aiding in prevention of similar attacks in the future.

First, to give you a brief overview on backdoor mitigation, efforts initially focused on poisoning attacks against worm signature generation and spam detectors. More recently, advances in AI explainability techniques have been utilized to generate clean-label poisoning attacks with small triggers targeting malware classifiers. 

Moving towards cybersecurity mitigations, one promising approach we've seen recently involves the use of an autoencoder-based intrusion detection system. This system operates under the assumption that malicious poisoning attack instances constitute less than 2% of total instances. Another model uses Principal Component Analysis (PCA) to protect against poisoning attacks.

Now, let's shift our focus to a different perspective. A recent study offers an interesting perspective on backdoor mitigation. That is to say, the paper argues that backdoors could coincide with naturally present features in the data. This suggests that further exploratory work is needed in order to accurately identify and remove these backdoor-triggering samples.

Let's discuss the taxonomy of Predictive AI, focusing on Poisoning Attacks and their respective mitigations. Firstly, we have Model Poisoning. This involves altering machine-learning or ML models to insert harmful functionality. It can be achieved either by modifying the trained model directly or by subtly manipulating the local model updates sent to the server, particularly in a federated learning context. Doing so leads to availability and integrity violations in federated models. For example, TrojNN is a reverse engineering technique that retrains the neural network model by incorporating its trigger into the external data to poison it. 

Similarly, poisoning can occur in supply-chain scenarios where attackers embed malicious code into the components or models provided by the suppliers, thus compromising them. Next, we have Availability Attacks. They aim to degrade the global model's accuracy and usually call for the adversary to control a considerable percentage of the clients.

Further, we have Targeted Poisoning and Backdoor Attacks that specifically induce integrity violations on chosen test samples. They achieve this by modifying local model updates aligned with a targeted objective or by introducing a trigger via malicious client updates. These are some key forms of poisoning attacks, and each requires individual attention and focused mitigation strategies.

The next strategy to poison models that we’ll explore is Byzantine-resilient aggregation rules. They are designed to identify and exclude harmful updates during the server-side aggregation process. However, we must note that these defenses could be circumvented by motivated adversaries. Moving on, we examine two more techniques known as Gradient Clipping and Differential Privacy. These may mitigate model poisoning attacks to a certain extent but it is important to remember that accuracy may have to be sacrificed for this. Model Inspection and Sanitization serve as useful strategies specifically for addressing model poisoning vulnerabilities like backdoor attacks. Program Verification Techniques, which are commonly used in fields such as cryptographic protocol verification, may also be applicable in preventing model poisoning. However, we are faced with challenges due to the inherent randomness and non-deterministic behavior in Machine Learning, or ML, algorithms. One of the key challenges we must address is the design of ML models to withstand supply-chain vulnerabilities. These strategies form the frontline of our defense against model poisoning, an essential concern for the safety and efficacy of our predictive AI systems.

Let's delve into the predictive AI taxonomy, specifically focusing on privacy attacks. Starting with reconstruction attacks, these are aimed at reverse engineering private information from aggregate data. This primarily includes user records and crucial infrastructure data. In recent times, such attacks have been adapted for binary and multi-class neural network classifiers. Now, let's move on to membership-inference attacks, which is another form of privacy violation. Here, the adversary's goal is to determine if a particular record was part of the dataset. Such breaches are often targeted at computing statistical information or training a machine learning model. Additionally, there are other significant privacy attacks that we must be aware of - model extraction attacks and property inference attacks. Model extraction attacks are designed to extract information about the ML model, for example, the architecture of the model or its parameters. On the other hand, property inference attacks aim to extract global information about a training dataset. All these types of privacy attacks pose a significant threat to the security of our data and should therefore be taken into serious consideration."

As we have seen, privacy attacks attempt to misuse aggregate information from user data in various ways, from data reconstruction and membership inference to model extraction and property inference. However, it's not all doom and gloom, we will also explore the countermeasures or mitigating strategies that can be implemented to combat these privacy attacks. Looking at the broader picture though, there are still many challenges in developing an overall effective mitigation strategy. As we go through this section, bear in mind these challenges and consider what potential solutions could be brought to the table.

Talking about taxonomy of predictive AI, we now focus on the privacy attacks and data reconstruction. We start with discussing data reconstruction attacks as one of the major concerns for privacy. These attacks have the potential to recover individual data from aggregated information. Initially these attacks required a multitude of queries for reconstruction. However, subsequent amendments have reduced this requirement, now only needing a polynomial number of queries.

We then shift our focus to the context of machine learning classifiers, where model inversion attacks come into play. These attacks aim to reconstruct class representatives from the training data of a machine learning model. However, it is essential to understand that they cannot directly reconstruct the exact training data used by the model.

Lastly, we touch upon the speculation around the capability of neural networks to reconstruct training samples. This might be due, in part, to these networks' remarkable tendency to memorize their training data. This memorization aspect, despite its implications, is crucial in attaining almost optimal generalization error in machine learning. As we move forward, we will explore more about these privacy attacks and their connection to the memorization characteristic of AI models.

Next, we will explore various types of Privacy Attacks in Predictive AI. Beginning with a data reconstruction attack example, a reconstructor network was trained to recover specific data samples from a neural network model, assuming an adversary possessed information about all other training samples. The discussion then shifts to a significant advancement in privacy attacks, demonstrating how training data from a binary neural network classifier can be reconstructed solely from model parameters. This approach was subsequently extended to reconstruct training samples from multi-class, multi-layer perceptron classifiers. The exploration continues with attribute inference attacks, where attackers extract a sensitive attribute from the training set with partial knowledge about other features. Lastly, the discussion touches on the ability of neural networks to memorize datasets, emphasizing the necessity of memorization for high-accuracy learning. Further details on these findings can be explored in the original paper.

Continuing our discussion, we explore data reconstruction attacks and memorization in predictive AI. A pivotal study highlighted how neural networks, as revealed by researchers, possess the ability to memorize randomly selected datasets. The emphasis was placed on the necessity of memorizing training labels for achieving near-optimal generalization error in Machine Learning.

Shifting the focus, researchers contributed by introducing two learning tasks—next-symbol prediction and cluster labeling. Notably, these tasks demonstrated that memorization was a fundamental requirement for achieving high-accuracy learning.

Advancing the discourse, researchers expanded on this concept by evaluating the significance of memorization for generalization through an influence estimation method. The examination of the interplay between data reconstruction attacks and memorization in generative AI promises to be a compelling topic and will be explored in subsequent discussions.


Let's look at the concept of Membership Inference within the domain of Predictive AI Taxonomy and its relation to privacy attacks. Membership inference attacks are directly linked to the privacy issues that can arise when we disclose aggregated information or when we train machine learning models using user data. 

These attack types can expose private details about a specific individual and can even serve as a stage for data extraction attacks. So, what does the attacker aim to do? The attacker's objective is to identify if a particular record or data sample was included in the dataset that was used for the statistical or machine learning algorithm. 

It's critical to understand that this kind of attack can lead to serious privacy implications. For instance, if an individual is proven to be part of a training set for a medical study of rare disease patients, it's already a breach of privacy. So, these attacks are vitally important to monitor and thwart when releasing aggregate information or machine learning models that are based on user data.

These attacks are mainly carried out against deep neural networks used for classification. The question arises: how do we measure the attacker's success in carrying out a membership inference? It has been defined by means of a game event inspired by cryptographic techniques, wherein the attacker and a challenger interact. The primary objective of this interaction is to ascertain if a targeted data sample was actually used while training the queried Machine Learning, or ML, model.

Two commonly used techniques for executing membership inference attacks are the Loss-based attack and the Shadow Models technique. While these techniques have notable complexity differences, they both offer comparable precision at low false positive rates, making them highly valuable in conducting privacy attacks.
We also have notable alternatives to these techniques, such as the LiRA attack, which assumes Gaussian model logit distributions for its operation. Another method has been proposed that only requires training a single model. While each of these techniques operates differently, they all contribute to the broad field of membership inference attacks within privacy attacks.
You can learn more about these attacks in detail in the original paper.

Let's focus on Membership inference attacks under the label-only threat model. Here, the adversary is only privy to the predicted labels of the queried samples. Membership inference attacks reveal private data about an individual in the same vein as reconstruction or memorization attacks, posing significant privacy issues when releasing aggregate data or machine learning models trained on user data. Knowing that an individual is part of the training set can have serious privacy implications - for instance, in a medical study involving patients with a rare disease. 

Moreover, Membership inference has potential usage as a foundation for launching data extraction attacks. In membership inference, the attacker's primary goal is to identify if a specific record or data sample was a part of the training dataset used for the statistical or ML algorithm. For those looking to familiarize themselves with implementing these attacks, there are several public privacy libraries, such as the TensorFlow Privacy library and the ML Privacy Meter, which provide ready-made implementations of these attacks.

In this segment, we discuss the idea of model extraction, which forms part of the predictive AI Taxonomy under the section of privacy attacks. Model extraction refers to a scenario where an attacker attempts to steal information about the model architecture and the parameters. This is done by submitting queries to the machine learning model, which has already been trained by a Machine Learning as a Service, or MLaaS, provider. It's critical to note, the very first model stealing attacks were witnessed on an array of diverse machine learning models.

Now, one important point is, while the extraction of exact models has proven to be an impossible task, it is still possible to acquire functionally equivalent models. These extracted models might differ from the original, but they manage to deliver similar performance in terms of prediction tasks.

Finally, model extraction should not always be viewed as the endpoint. On many occasions, it serves as a stepping stone or the preliminary step towards launching more potent attacks. Hence, understanding and preventing model extraction can in turn aid in mitigating more intense, downstream attacks that rely on the attacker having knowledge of the model architecture and weights.

There are three methods employed by attackers for model extraction. The first technique relies on direct extraction based on the mathematical formulation involved in deep neural networks' operations. This allows the adversary to compute model weights in a more algebraic manner. The second technique leans heavily on the usage of learning methods for extraction. For instance, active learning guides the queries to the machine learning model for more efficient extraction of model weights. Furthermore, reinforcement learning can train an adaptive strategy that effectively reduces the number of queries required for extraction. The third technique for model extraction involves the use of side channel information, such as electromagnetic side channels or rowhammer attacks to extract models of complex neural networks. It's crucial to mention that these model extraction tactics are not typically the end goal, but rather a step towards mounting other, more potent attacks. As attackers gain knowledge of the model weights and architecture, they can launch more powerful attacks. Therefore, prevention of model extraction is a significant way to mitigate downstream attacks that would otherwise depend on the attacker possessing knowledge of the model architecture and weights.

Let's delve into Property Inference, a category of privacy attacks. In a property inference attack, the attacker learns global information about the data distribution used for training an ML model. This suggests that the attacker can infer sensitive attributes from the training set, attributes which might be confidential and not meant to be disclosed. The concept and formalization of these attacks were initially introduced and demonstrated on various ML models, including hidden Markov models, Support Vector Machines, and neural networks among others. Studies, including those by other researchers, have indicated that the property inference attacks can be made more effective by poisoning the property of interest, thus leading to a more efficient distinguishing test. Other researchers even designed an efficient method to estimate the size of the targeted population in an attack.

It is also important that we discuss the topic of Property Inference and how models are under attack. Property inference attacks have been perpetrated against various ML models. The list is quite extensive and includes Hidden Markov models and Support Vector Machines. We also find these attacks on Feed-forward neural networks, Convolutional neural networks, and Federated learning models. 

Other models that are particularly susceptible to Property Inference attacks are generative adversarial networks and graph neural networks. Understanding the variety of models and their susceptibility to these attacks can help us devise better protective measures and improve the overall security of our ML models.

Here, we are discussing the privacy attacks and respective mitigations. We begin with the concept of reconstruction attacks against aggregate information, which led to the creation of differential privacy or DP. By providing a limit on what an attacker can gather about each record in a dataset from the output of a certain algorithm, DP ensures robust privacy protection. 

Furthermore, DP has become a preferred choice due to several advantageous features, such as group privacy, post-processing, and composition. Common DP mechanisms include the Laplace, Gaussian, and Exponential techniques. Meanwhile, DP-SGD stands out as the most popular DP algorithm for training machine learning models. 

The effectiveness of DP extends to mitigating against data reconstruction and membership inference attacks. However, it's important to note that DP does not guarantee total protection, particularly from model extraction attacks. As we continue our exploration of predictive AI taxonomy, the nuances of these concepts and their effects on privacy protection will become clearer.

One of the primary challenges related to using Differential Privacy pertains to a balance between privacy and utility, commonly gauged as an accuracy measure for Machine Learning models. Typically, this necessitates the use of larger privacy parameters, making it difficult to precisely quantify the actual privacy benefits achieved.

Now let's discuss a promising recent methodology, privacy auditing. This technique aims to measure the real-world privacy guarantees and assign privacy lower bounds by setting up privacy attacks. These attacks can include membership inference attacks, but it has been found that poisoning attacks often enable more accurate estimates of privacy leakage.

We also delve into various efficient methods for conducting privacy auditing while training a single model. This includes using multiple random data canaries and client canaries, supplemented by cosine similarity test statistics to conduct an audit of user-level private federated learning. Through this slide, we aim to explore such practical applications and challenges in the sphere of Differential Privacy, and the promising solutions on the horizon.

There are several techniques we can employ to counter model extraction. These include limiting user queries to the model, detecting suspicious queries, and creating more robust architectures to prevent side channel attacks. However, we must be aware that these can potentially be circumvented by motivated attackers. Let's now examine a very different approach. This one is called machine unlearning. This innovative technique enables users to specifically request the removal of their data from a trained ML model. We categorize machine unlearning methods into two: exact and approximate. Exact machine unlearning necessitates retraining the model altogether. On the other hand, approximate methods update the model parameters to reduce the influence of the unlearned records. For ensuring robust security in machine learning deployments, we have at our disposal numerous practice guides. They offer detailed instructions and best practices to secure machine learning operations.

Congratulations! You have completed the module on Predictive AI Taxonomy. Continue to the next module of Generative AI Taxonomy in Adversarial Machine Learning.