Hello! Welcome to module 3 of the NIST's Adversarial Machine Learning course. In this module, we will be introducing you to Generative AI Taxonomy. I hope you have covered the previous modules on Introduction to AML and Predictive AI Taxonomy. If not, do check out those modules! In this module, we will be discussing about attack classification, AI Supply Chain attacks

Let's dive into the taxonomy of Generative AI. Generative AI is a specialized type of AI technology that builds models capable of generating content similar to their training data. This content can span various mediums, such as images, texts and a wide range of other media. A variety of AI technologies, such as generative adversarial networks, Generative Pre-Trained Transformer, and Diffusion Model come under the umbrella of Generative AI. The landscape of Generative AI is continually evolving. One prominent update is the emergence of multi-modal AI systems, which blend two or more technologies together. This combination unlocks the capability of multi-modal content generation, bringing a whole new dimension to content creation.

We're going to start with generative adversarial networks, or GANs, which are intriguing AI models where two networks - a generator and a discriminator - are in a constant competition with each other.
Next, we will discuss the Generative Pre-Trained Transformer, or simply GPT. It's an AI model that leverages transformer models for generating coherent and grammatically correct sentences. Finally, we will touch upon Diffusion Models, which are a unique type of generative model. These models generate new instances based on the very distributions they've learned from their training data. This information will provide us with a solid understanding of the diverse technologies within Generative AI and will help us explore how they are forming the core of multi-modal content generation capabilities.

Let’s delve into the exciting area of recent advancements in Generative AI, more specifically, the emergence of multi-modal AI systems. These systems are remarkable as they are able to integrate two or more technologies, thus enabling them to possess multi-modal content generation capabilities. This basically means, instead of generating just one type of output such as images or text separately, they can concurrently generate multiple forms of output. For example, an AI model could generate a text description of a movie scene, as well as a sketch. This is a significant advancement indicating the growing complexity and capabilities we are seeing in the field of Generative AI. This evolution is not just about more sophisticated generative models, but importantly, about systems that can handle and integrate multiple forms of data to generate richer and more complex outputs.

Further, we will discuss the taxonomy and attack classification of Generative AI, or GenAI. Recent work has shed light on the unique security risks posed by GenAI, hence emphasizing the need to understand the novel security violations. Let's delve deeper into this area by focusing on the rational categorization of attacks. In adversarial machine learning for GenAI, we categorize attacks based on what the attacker intends to accomplish. The primary objectives are availability breakdowns, integrity violations, privacy compromise, and abuse violations. Each category of attack is significant and requires our attention. 

Moving on, we can divide the attacks even further. This division is based on the learning stage the attack relates to and the knowledge and access the attacker possesses. The type of learning stage an attack applies to and the attacker's knowledge and access could open up new dimensions in understanding potential threats to GenAI. In the following sections, we will take a closer look at how these categories influence and shape the pandora's box of GenAI security attacks.

We will delve into novel security violations and how they apply to Generative AI, or GenAI. We will focus specifically on the adversary's capabilities and attack classifications needed to achieve their objectives. Each of these capabilities are highlighted in the outer layer of objective circles.

Similar to the PredAI taxonomy in the previous module, this taxonomy is first categorized by the attacker’s objectives, which include availability breakdowns, integrity violations, and privacy compromise. For GenAI systems, violations of abuse are also especially relevant. The capabilities that an adversary must leverage to achieve their objectives are shown in the outer layer of the objective circles. Attack classes are shown as callouts connected to the capabilities required to mount each attack. Multiple attack classes that require the same capabilities to reach the same objective are shown in a single callout. 

Certain areas are intimately linked to language models. For instance, Retrieval Augmented Generation, or RAG, is paramount to many attacks during the deployment stage. Shifting our attention to attack types, it's crucial to note that many attack types are equally applicable to Predictive AI and Generative AI. However, due to the increasing body of research indicating potential security concerns with GenAI, we find it necessary to give special attention to security violations in GenAI. Despite the apparent overlap, this unique focus is truly merited due to the distinctive threats that arise with the use of Generative AI.

Let’s look at the taxonomy of Generative AI (GenAI) and how it's vulnerable to various types of attacks. There are two key stages in the development of GenAI models: the creation of a foundation model and its subsequent fine-tuning for specific applications. It is significant to know that this process makes GenAI models susceptible to poisoning attacks, wherein an adversary can manipulate a portion of the training data to cause intentional and targeted model failures.

Furthermore, we also focus on inference-time attacks that occur during the deployment phase of GenAI models. This is when information is manipulated in a way that is somewhat parallel to SQL injections. 

We then highlight five key characteristics of deploying GenAI models. First, is the alignment of the model instructions, which basically provide a roadmap for the model. Next, we have contextual few-shot learning which is a process that improves application performance by providing specific examples of expected inputs and outputs. 

The next point of focus is runtime data ingestion from third-party sources. This refers to the injection of external data into the model at runtime, for tasks like summarizing documents or web pages. Then, we talk about output handling which concerns how the model's output is utilised, such as populating a webpage or constructing a command. 

Lastly, we discuss the use of agents. They provide additional context to the model's input by processing the output, and often perform additional tasks based on the information provided.

In this part of our Generative AI Taxonomy and Attack Classification, let's discuss five key aspects we deal with. The first one is Alignment via model instructions. This refers to instructing the GenAI model's behavior at inference time using pre-defined prompts. These prompts can be overridden by attackers, potentially resulting in model compromises. 

The second point we'll discuss is Contextual few-shot learning. With this method, LLMs, which can also be referred to as Generative AI models, are improved by providing examples of the inputs and outputs expected for the application prior to running it. This teaches the model to carry out tasks in a more natural manner.

Our third point is Runtime data ingestion from third party sources. Here, we're talking about indirect prompt injection attacks, where the attackers alter the context using data from external sources. 

Next, we'll talk about Output handling. The output of GenAI models has varied uses. For example, it could be used to populate elements on a web-page or construct operational commands. 

Finally, we'll discuss Agents. Agents are systems that process the output from GenAI models to accomplish additional tasks and provide added input context. These agents can also be susceptible to attacks where the attacker changes their configuration in natural language.

In this segment, we delve into the Generative AI Taxonomy and Attacker Goals and Objectives. We categorize attacker objectives along three significant dimensions which are - availability, integrity, and privacy. However, there is a fourth, unique objective when it comes to Generative AI: abuse violations. Exercising the principle of abuse violations, an attacker repurposes a GenAI system's intended use to achieve their personal objectives. To shed light on how this plays out in real-world scenarios, attackers can exploit GenAI models to propagate hate speech, churn out media tailored to incite violence, or even expand on offensive cybersecurity operations.

Now, we are concentrating on the attacker capabilities in the taxonomy of Generative AI (GenAI). Let's dive into some of the common types of attacks, focusing on three primary areas where attackers tend to exploit. We start with Training data control. In this scenario, an attacker manipulates a part of the training data by either inserting or changing samples. This is common in data poisoning attacks. Next is Query access. This type of attack is often seen with GenAI models that are hosted in a cloud environment. What happens here is that, by submitting specific inputs to the model, the attacker can trigger specific model behaviors, such as prompt injection, extraction, or even stealing the whole model. Finally, we have Source code control attacks. In these cases, attackers can modify crucial parts of the machine learning algorithm, like the random number generator or third-party libraries, causing a variety of security issues. Therefore, understanding these attack classifications is important to secure Generative AI systems successfully."

Moving on to the continuation of Generative AI Attacks, specifically in terms of certain attacker capabilities. The rise of open source repositories, a prime example being HuggingFace, presents an increased risk. These platforms can potentially be exploited by attackers to create what are known as 'malicious models,' or even sabotage completely harmless models through the insertion of dangerous or harmful code. 

Now let's delve into 'resource control.' This scenario comes into play when attackers manipulate resources, these could be documents or web pages that the GenAI model depends on. The aim here is to execute what we call indirect prompt injection attacks. Manipulating these resources effectively allows the attacker to infiltrate the GenAI system from an indirect angle, making it potentially tricky to identify and handle.

We will now review the taxonomy of Generative AI and discuss AI Supply Chain Attacks. The main point to take away is that security vulnerabilities against Machine Learning need to be addressed from a comprehensive standpoint. This means factoring in all aspects, including the software itself, data, model supply chains, network and storage systems. AI, since it is essentially software, unsurprisingly inherits the vulnerabilities inherent in the traditional software supply chain. It's crucial to understand that much of the tasks related to Generative AI usually initiate with open-source models or data presiding outside the realms of traditional cybersecurity. In real world scenario, repositories such as TensorFlow and OpenCV that are significant in the field of machine learning, unfortunately, have a high exposure to vulnerabilities.

It's essential to understand that a lot of Machine Learning, or ML, projects initiate by incorporating an open-source GenAI model for the downstream application. These models are typically stored in various formats such as pickle, pytorch, joblib, numpy, or tensorflow. All these formats provide serialization persistence mechanisms. However, this process of serialization persistence can potentially have serious security implications. It could lead to something known as arbitrary code execution, or ACE, upon deserialization. This phenomenon is generally seen as a critical vulnerability. In fact, we have observed deserialization vulnerabilities in tensorflow and pickle in neural network tools, identifiable via their respective CVE IDs.

Here, we are discussing the risks associated with GenAI. As we know, the performance of GenAI models is greatly influenced by the size of the model, the scale of the dataset, and the quality of that dataset. One prominent issue we are facing is that while dataset publishers provide URLs for datasets, these URLs are potential targets for compromises, leading us to the threat of poisoning attacks. The attackers can replace the resources, resulting in targeted poisoning attacks and model poisoning. A mitigation approach that is often proposed is for datasets to have both the URL and a cryptographic hash, a tool for verification. However, we must note that this solution might not be universally applicable; it may not scale well for larger distributed datasets available on the internet.

When we delve deeper into GenAI's mitigation techniques, we are confronted with a few challenges. One common practice among GenAI foundation model developers is the scraping of data from a wide array of uncurated sources. However, these sources are susceptible to a variety of threats. One such threat comes from expired or purchased domains that serve dataset URLs, potentially giving rise to targeted poisoning attacks, backdoor poisoning attacks, and model poisoning. Cryptographic hashes are a feasible solution for verifying the content that has been downloaded, but their effectiveness diminishes when applied to large distributed datasets available on the internet. More details about this issue can be found in the next module, which provides an in-depth analysis.

"The topic of discussion is Generative AI Taxonomy, with a focus on AI Supply Chain Attacks and possible mitigation strategies for these attacks. Mitigation strategies hinge on confidence practices applied throughout the supply chain. For dependencies related to model files, regular vulnerability scanning of model artifacts is key. This should take place within the ML pipeline and it's advisable to utilize secure model persistence formats such as safetensors. In terms of web data dependencies, it's critical to verify web downloads carefully. We can accomplish this by publishing and subsequently verifying the cryptographic hashes of training data. This strategy can help prevent domain hijacking. Another mitigative technique we can employ pertains to image files. We can 'immunize' these files which makes them resistant to potential manipulation by large diffusion models. However, it's important to note that this method necessitates an additional policy.

We're now discussing a specific aspect of Generative AI Taxonomy, which is Direct Prompt Injection Attacks and their Mitigations. Direct prompt injection is a tactic where user text is intentionally designed to modify or alter the behavior of the Large Language Model, or LLM. 

Now, let's delve into the primary goals of an attacker opting for this strategy. The first goal is abuse, where attackers employ prompt injection tactics to circumvent safeguards and produce misleading, harmful, or even sexual and phishing content, this act is often referred to as a 'jailbreak'. 

The second key goal of attackers is to invade privacy. Here, the intention is to extract system prompts or expose personal information that has been previously provided to the model. It's paramount to understand that these attacks are substantial privacy and security threats.

Continuing our discussion on Generative AI Taxonomy and Direct Prompt Injection Attacks and Mitigations, let's dig a little deeper and examine one particular goal of these attacks, which is invading privacy. This goal is of utmost significance because it involves an intrusive attempt by attackers to expose private information of users, information that wasn't meant for unrestricted access. It's crucial to understand this aspect as these threats can lead to significant violation of privacy of unsuspecting users. We will delve further into this topic of privacy invasion in the context of direct prompt injection attacks in the later parts of our module. So, please look forward to that in-depth examination.

I'll share insights on key attacker techniques for launching direct prompt injection attacks on Generative AI, which typically fall into several categories. Firstly, we have Gradient-based attacks. These are effectively white-box optimization methods designed for direct prompt injection attacks. The goal of these attacks is to minimize the changes in the language, while maintaining fluency and perceptibility. Notable examples of these attacks include HotFlip and Universal adversarial triggers. These techniques have shown to be easily transferable to other models, which exposes open-source models as feasible attack vectors.

Second, let's discuss the Manual methods. These strategies capitalize on the model's vulnerability to linguistic manipulations and often come in two flavors. The first is known as competing objectives. This includes methods like Prefix injection, Refusal suppression, and Style injection. These techniques essentially involve prompting the model in a way that competes with its existing programming, in an attempt to manipulate its output. The second strategy is known as mismatched generalization, which involves positioning inputs far from the model's standard training data, causing it to diverge from its safety protocols or guardrails.

Lastly, we've got Role-play strategies, which include tactics like 'Do Anything Now' (DAN) and 'Always Intelligent and Machiavellian' (AIM). These strategies essentially aim to exploit the model's adaptability to various roles or characteristics. Typically, these tactics guide the model to adopt certain personas or behavioral patterns that contravene its original intent, potentially compromising its ability to adhere to safety protocols. Together, these strategies constitute the primary taxonomy and attacker techniques that potentially threaten Generative AI models.

Let's now look into the details of manual methods for launching GenAI attacks. These techniques can mainly be classified into two categories, competing objectives and mismatched generalization arising from certain linguistic manipulations. 

In the category of competing objectives, we have three methods of additional instruction that override the original model instructions. 

The first method, prefix injection, involves guiding the model to start responses with an affirmative confirmation. The aim here is to influence the subsequent language generation of the model.

The second method involves refusal suppression. In this case, we provide explicit instructions to the model to avoid generating refusals or denials in its output. This forces the model to comply with the provided instructions.

The third method, style injection, instructs the model to limit the sophistication or accuracy of the model's responses. This is achieved by restricting the language to simpler tones and limiting the use of professional language.

In the category of mismatched generalization, we have ways to alter the input data presentation and manipulate input text.

The technique of special encoding alters the representation of input data, making it unrecognizable to standard recognition algorithms. 

While character transformation manipulates characters of the input text obscure to the original meaning. 

The word transformation technique alters the linguistic structure through methods like Pig Latin, synonym swapping, or payload splitting.

Lastly, prompt-level obfuscation introduces ambiguity or altered linguistic contexts to the model's input. It is used to create scenarios in which the model's safety mechanisms are less effective.


Let's delve into one of the attacker techniques in direct prompt injection attacks, that is, Automated Model-based Red Teaming. This approach employs three models which include an attacker model, a target model, and a judge. The significance of this approach becomes evident when the attacker has access to a high-quality classifier. This classifier's role is to judge if the model output is harmful and can hence be used to generate jailbreaks. Key to note is that this strategy requires only query access for each of these models, so human intervention is not necessary to update or refine a candidate jailbreak. In practice, such algorithms may be exponentially more query-efficient than existing algorithms, they may need only dozens of queries per successful jailbreak. An interesting point to focus on is that the prompts these algorithms generate have been demonstrated to be transferable from the target model to other closed-source Large Language Models or LLMs.

We are now focusing on how Generative AI models, or GenAI models, can be vulnerable to data extraction attacks. Essentially, these attacks enable perpetrators to extract confidential data that these models have been trained on or have been supplied with during the completion of their designated tasks. Let's look at an example. 

Data extraction attacks in the domain of GenAI language models may result in the exposure of sensitive details. How does this happen? If the model is fed prefixes or pieces of textual data that comprise private details, the model can inadvertently complete it, thereby leaking sensitive data. This could involve, for instance, revealing email addresses, phone numbers, and geographical location details. 

It's key to understand that these data extraction attacks pose a serious threat as they may lead to significant security breaches by leaking sensitive information.

A key element to discuss here is prompt and context stealing. For those who may not be familiar, prompts that are carefully crafted are integral in aligning Large Language Models, or LLMs for short, with their specific use-cases. This makes these prompts very attractive targets for those with malicious intent. Theft of prompts can yield detrimental results, including risking the intellectual property and privacy of engineers or even potentially jeopardizing an entire business model. It's worth noting that there are tools, such as PromptStealer, that can manipulate GenAI models and reconstruct the stolen prompts. The process doesn't stop there. There are a variety of techniques that can be used to extract sensitive information straight from a GenAI model's context. For instance, imagine a model that has been trained with a PDF document or database rows. This can be manipulated to reveal those very details via direct prompt injections. As we continue with this presentation, we'll further examine these potential threats to information security and discuss how to mitigate these risks.

Next, we focus on Direct Prompt Injection Attacks and their mitigations. The first mitigation strategy is the training for alignment, which aims to strengthen built-in mechanisms against such attacks. This method involves careful tuning of model alignment training using carefully curated and pre-aligned datasets. The process can be further optimized using reinforcement learning, in which human feedback proves pivotal. 

Shifting to the second strategy, we can see that prompt instruction and formatting techniques serve as an effective approach as well. Users of the model can be instructed in a way that makes them treat user input more cautiously, minimizing chances of potential security threats. 

Finally, let's consider detection techniques which involves a more stringent approach towards backward alignment. It utilizes benchmark datasets or filters that constantly monitor the protected Language Model's input and output, thus ensuring more secure use. In summary, these mitigation strategies, while differing in their approach, collectively aim to counter the threat of Direct Prompt Injection Attacks efficiently.

In this section, we're focusing on further mitigations and defenses. There is a strategy of evaluating distinctly prompted Language Learning Models (LLM) to identify any adversarial prompts. This method assists in distinguishing potentially harmful prompts. In the same vein, there are commercial products available offering tools to detect such prompt injections. These tools are capable of identifying potential malicious user input and regulating the output of the firewall to prevent any harmful behavior known as 'jailbreak'. Lastly, another aspect under development is the defenses for prompt stealing. The reliability of these defenses is not yet confirmed as they are still under development. This often involves looking at the model utterance and comparing it to the prompt. These steps in mitigations and defenses are all proactive measures taken to ensure the integrity of our AI systems.

The focus of this slide is on Indirect Prompt Injection Attacks. We see that Retrieval Augmented Generation is viewed as a primary use case for Language Model Learners or LLMs. Interestingly, this brings certain attacker techniques into play which manipulate data channels to impact system operation. An interesting parallel can be made here with SQL injection attacks. However, the key differentiating factor is that these techniques do not call for a direct interaction with the LLM. Instead, they utilize something identified as resource control. This enables an attacker to indirectly, even remotely, inject system prompts without engaging directly with the RAG application. It's crucial to note that these indirect prompt injection attacks can potentially pave the way for violations across various sectors such as availability, integrity, privacy, and abuse categories.

We are going to delve into the Generative AI Taxonomy, specifically focusing on availability violations and peculiar attack techniques. Let's start with model availability violations. These disruptions in service are often incited by attackers using malicious inputs to prompt a model. This leads to an increase in computation or an overload on the system, resulting in a denial of service to its users. Attackers often execute these attacks using indirect prompt injections, making a system resource, instead of a registered user, the source of violation. Such attacks can make the model perform slowly or even render it unusable, blocking certain capabilities.

Now, let me share some examples of such attacks. Researchers have performed attacks on a commercial RAG service using indirect prompt injection, where a system resource contained certain instructions. The first one being Time-consuming tasks. In this attack, the model is made to perform a long-duration task before responding, which even involves looping behavior in some models. 

Secondly, the Muting technique is exploited to make the model incapable of finishing sentences when an early end of text token appears. It's clear that these attacks aim to disrupt the services provided by the model, thereby affecting its performance and usability.


We will focus on the topic of Integrity Violations, a crucial aspect that often leads to problems like trust issues in GenAI systems. For instance, systems like Bing developed by Microsoft and Bard by Google unknowingly spread misinformation. The critical point is that integrity breaches are not always due to system errors, but they can be orchestrated. Researchers have demonstrated that by manipulating the primary task of the Language Learning Model or LLM, integrity attacks can occur. This situation differs from the usual indirect prompt injection attacks where the system performs malicious secondary tasks. The manipulation attacks signify a considerable threat as these anomalies can cause the model to provide incorrect summaries and even propagate disinformation. One of the methods involves relying on untrustworthy news sources or outputs from other chatbots.

Next, let me share some examples of manipulation attacks, a type of integrity violation, within the taxonomy of Generative AI. To give you an understanding, let's start with the first example - the arbitrary wrong summaries. In this scenario, a model can be maliciously prompted to produce adversarial summaries. This manipulation may involve presenting skewed summaries of documents, emails or even search queries. This is a good example of how the primary task of the Language Learning Model can be manipulated to produce untrustworthy outputs. 

Moving on to the second example, we discuss the propagation of disinformation, typically through search chatbots. Instead of providing accurate information, these chatbots can be prompted to spread disinformation. This is typically done by relying on untrustworthy news sources or manipulating the outputs from other search chatbots. These instances highlight the vulnerabilities in Generative AI systems, and the lengths to which attackers may go to exploit these vulnerabilities.

In this section, we're going to delve into Generative AI Taxonomy and a significant risk associated with it - Indirect Prompt Injection Attacks. These attacks present a multitude of privacy complications. Let's consider Italy's stance on this as an example. Italy was one of the first nations to outlaw the use of a known AI, ChatGPT. The primary reason behind this action was the fear of allowing access to sensitive information and chat logs. As we look deeper into this issue, we realize that an attacker's main motivations can be split into two vital categories. The first one is Information Gathering. Certain targeted attacks, like indirect prompting involving human involvement, can heighten these risks by gleaning user data, leaking their chat logs, and, in some instances, extracting other sensitive information. Let's consider an example where human intervention is not required. Suppose there is an attack on a personal assistant, an AI that has access to a user's data. Even in this situation, the privacy concerns are no less critical than the earlier scenario.

Further, we are discussing unauthorized disclosure and attacks on Generative AI which are a prominent concern in this age where models are being increasingly integrated into system infrastructures. When this happens, we often see unauthorized disclosures or even access to private user data. This issue becomes an enticing goal for cyber attackers, who want to exploit the vulnerabilities for their own advantage. Now, how do they do this? There are different techniques these attackers can utilize to exploit these disclosure avenues. They can launch backdoor attacks and gain access to large language models, or LLMs, and other systems. Various methods like issuing API calls or malicious code auto-completions can be used for this purpose. So, the risk facing us in the world of Generative AI is twofold, not only do we have to prevent unauthorized disclosure of data, but we also have to safeguard the systems from potential backdoor attacks.

Let's dive into the taxonomy of Generative AI in relation to indirect prompt injection attacks and mitigations. There are a few different types of attacks that we need to be aware of. Firstly, we have human-in-the-loop indirect prompting. This method uses read operations, such as triggering a search query, to transmit information to the attacker. 

Next, we have the method of interacting in chat sessions. In this scenario, the AI model convinces the user to follow a URL. The attacker then uses this URL to insert the user’s name, leveraging a sophisticated form of phishing. 

Lastly, let's consider invisible markdown image attacks. This is a prompt injection attack on a chatbot, where the attacker modifies the chatbot's response with an invisible, single-pixel markdown image. This image siphons off the user's chat data and funnels it to a malicious third-party. 

These are just a few examples of how attackers can exploit AI systems and pose significant privacy risks. Understanding and mitigating these risks is crucial when working with generative AI systems.

Let's delve deeper into the realm of Generative AI, particularly touching upon the concept of abuse violations. GenAI introduces an intriguing new layer to the taxonomy pertaining to the attacker's aims aptly called abuse violations. Essentially, this is when an attacker repurposes a system's intended function to fulfill their own ends — this is typically achieved through something we call indirect prompt injection. 

Shifting gears, let's break down the concept of abuse violations. They primarily fall into three categories namely: Fraud, Malware, and Manipulation. Each has its unique characteristics and implications. 

In the scenario of fraud, advanced instruction-following LLMs unintentionally pave the way for dual-use risks. With Malware, the propagation of malicious links is alarmingly easy with LLMs. Another angle to be vigilant about is the integration of LLMs into applications, opening new avenues for malware threats. 

Finally, we look at Manipulation. Here, models serve as a vulnerable layer amidst the users and information outputs. With the increasing integration of LLMs into larger systems, susceptibility to manipulation soars. Particularly alarming are manipulation attempts where models are prompted to deliver skewed or entirely inaccurate summaries of information. Users tend to be credulous towards these LLMs, making them more susceptible to these manipulation attempts. All these categories highlight the potent risks associated with abuse violations in GenAI.

Next, we have Fraud. Recent developments in instruction-following LLMs come with a downside, they have spurred a rise in dual-use risks. Simply put, these technological advancements can be used for legitimate purposes as well as illicit activities. Next up is Malware. Given these AI models can suggest links to the users, it opens up room for spreading malware through malicious links. The situation is exacerbated when we consider the ubiquity of LLM-integrated applications as they present new risks. For instance, they can yield malicious prompts, a case in point is LLM-augmented email clients. These can deliver harmful prompts which are then circulated via emails.

In discussing abuse violations under the topic of indirect prompt injection attacks, we come to a significant category; manipulation. Here, generative AI models, or Language Learning Models (LLMs) as they are also known, play a pivotal role. They function as intermediary layers sitting between the users and the output of information. This setup exposes these models to easy manipulation, opening up a wide array of potential vulnerabilities. 

Let me share an example. Consider search chatbots, primarily used to generate or source information. Now, these can be exploited or manipulated, to produce disinformation. We can also see instances where they have been prompted to obscure certain specific pieces of information. Furthermore, they may be made to provide summaries of information sources that are intentionally inaccurate, adversarial, or completely wrong. 

Now imagine the extent of the risks when users, unaware of these manipulations, begin to unknowingly trust these compromised sources. This trust stems from LLMs' seemingly impartial nature and their authoritative tone, often leading users to fall victim to these manipulation attempts more frequently.

Let's now turn our attention to Generative AI Taxonomy and Attacker Techniques. Some dangerous misuses of AI have been observed and tested with chatbots. The first of these is Phishing. Large Language Models, or LLMs, have been shown to not only create convincing scams, but are also capable of spreading them widely and rapidly. 

Secondly, we have Masquerading. In this scenario, LLMs can mimic approved requests from service providers, or even suggest untrustworthy websites as being secure and reliable. 

Lastly, there's the technique of Spreading Injections. This is where the LLM acts in a similar way to a computer, running and propagating harmful code which can be harmful to your systems. 

These techniques demonstrate the potential misuse of AI technologies, emphasizing the need for extensive safeguarding and monitoring measures to mitigate such risks.

Let's continue our review of attacker techniques in Generative AI Taxonomy. We'll see three more notable examples. 

First, we have the method of spreading malware. Large Language Models, or LLMs, can actually assist attackers in directing users toward malicious websites that start harmful downloads instantly - a process known as 'drive-by downloads'. 

Next, we have historical distortion. In this kind of attack, the culprit might induce the AI model to produce false information, which has been deliberately chosen to misguide or mislead the user. 

Lastly, we come across marginally related context prompting. Here, the attacker achieves bias amplification by manipulating search results to point towards certain orientations. This inherently sideswipes any neutral interpretations. 

So, these are just a fraction of the multiple ways in which AI can be leveraged to execute attacks, demonstrating the crucial need to keep updating and enhancing security measures to protect our systems and prevent such incidents.

Our next topic of discussion is the various mitigation techniques that have been proposed for indirect prompt injection attacks. While these methods can significantly reduce model risk, they do not provide absolute immunity against all attacker techniques. Let's examine these different strategies.
Firstly, we consider employing Reinforcement Learning from Human Feedback or RLHF as a valuable mitigation technique. In RLHF, we involve humans indirectly to fine-tune a model, which can result in better alignment of LLMs with human values and thereby prevent undesirable behaviors.
Next, we augment our mitigation arsenal with input filtering. This method, proposed by Greshake et al, involves processing retrieved inputs to weed out suspicious instructions.
Last but not least, we have the concept of an LLM moderator. This approach aims at detecting attacks that go beyond just screening clearly harmful outputs, which boosts our resilience against prompts injections. It's worth mentioning that none of these mitigation techniques offer a comprehensive or foolproof solution against adversarial prompting, but they undoubtedly enhance model security and warrant further investigation for efficiency.

Next, we particularly focus on additional mitigations and challenges in the context of indirect prompt injection attacks. We're continuing our discussion on mitigation techniques, with the spotlight on interpretability-based solutions that have been employed for the detection of outlier predictions. Now, these solutions have actually shown promise in finding anomalies in input through the assessment of prediction trajectories. Despite these ongoing efforts, it's important to note that we've yet to come across a fully comprehensive or foolproof method that protects AI models against adversarial prompting. This is an undeniable challenge, but I believe it's not insurmountable. Future research and work should continue along this path, investigating and evaluating these suggested defenses for their efficacy. Remember, the ultimate goal is to ensure we're taking the necessary steps to mitigate risks while reaping the benefits of Generative AI.

Congratulations! You have completed the module 3 on Generative AI Taxonomy of NIST's Adversarial Machine Learning. There’s one final module remaining that discusses the remaining challenges. Great work!